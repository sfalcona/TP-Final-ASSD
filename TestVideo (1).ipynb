{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Video Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import ImageGrab\n",
    "\n",
    "cap = cv2.VideoCapture('VideoSample.mp4')\n",
    "cont = 0\n",
    "maxArea = 300\n",
    "prevC = 0\n",
    "danger = False\n",
    "bgs = cv2.createBackgroundSubtractorMOG2(10,80,True)\n",
    "while(True):\n",
    "    \n",
    "    ret, frame = cap.read()    \n",
    "    frame2 = bgs.apply(frame)\n",
    "    blur = cv2.GaussianBlur(frame2,(5,5),1) \n",
    "    ret, thresh_img = cv2.threshold(blur,91,255,cv2.THRESH_BINARY)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT,(5,5))\n",
    "    dilated = cv2.dilate(thresh_img,kernel,iterations = 6) \n",
    "    gradient = cv2.morphologyEx(thresh_img, cv2.MORPH_GRADIENT, kernel)\n",
    "    opening = cv2.morphologyEx(dilated, cv2.MORPH_OPEN, kernel)\n",
    "    contours =  cv2.findContours(opening,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)[-2]\n",
    "    \n",
    "    font = cv2.FONT_HERSHEY_COMPLEX\n",
    "    color = (0,0,255) if ((prevC > 2) and danger ) else (0,255,0)\n",
    "    danger = False\n",
    "    cv2.putText(frame,'.',(10,10), font, 4,color,5,cv2.LINE_AA)\n",
    "    \n",
    "    for c in contours:\n",
    "        if cv2.contourArea(c) > maxArea:\n",
    "            if prevC != 0:\n",
    "                danger = True\n",
    "                cv2.drawContours(frame, [c], -1, (0,255,0), 3)\n",
    "#                 [x,y,w,h] = cv2.boundingRect(c)\n",
    "#                 cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,255),2)\n",
    "        prevC = len(c)\n",
    "   \n",
    "\n",
    "\n",
    "    cv2.imshow('frame',frame) \n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from functools import reduce\n",
    "\n",
    "class myFilter:\n",
    "    '''Filtros FIR aplicados a imagenes.'''\n",
    "    def __init__(self,coef:'Coeficientes del filtro',size:'Tamano de la imagen, por ahora un solo canal (BN)'):\n",
    "        '''Inicializo filtro ingresando coeficientes del mismo y tamano de la imagen'''\n",
    "        self.coef = coef\n",
    "        self.prevFrame = deque([[0 for i in range(size)] for i in range(len(coef))])\n",
    "        \n",
    "    def applyFilter(self,newFrame:'Nuevo frame del video'):\n",
    "        '''Aplico filtro a la nueva entrada, devuelvo valor de la salida en mismo formato'''\n",
    "        self.prevFrame.pop()\n",
    "        self.prevFrame.appendleft(newFrame)\n",
    "        self.out = [[] for i in newFrame]\n",
    "        for i, pixel in enumerate(newFrame):\n",
    "            self.out[i] = reduce((lambda x, y: x + y),[self.coef[j]*self.prevFrame[j][i] for j in range(len(self.coef))])\n",
    "            \n",
    "    def __call__(self):\n",
    "        '''Devuelvo salida actual del filtro'''\n",
    "        return self.out\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading model...\n",
      "[INFO] starting video stream...\n",
      "[INFO] elapsed time: 26.72\n",
      "[INFO] approx. FPS: 8.35\n"
     ]
    }
   ],
   "source": [
    "# USAGE\n",
    "# python real_time_object_detection.py --prototxt MobileNetSSD_deploy.prototxt.txt --model MobileNetSSD_deploy.caffemodel\n",
    "\n",
    "# import the necessary packages\n",
    "from imutils.video import VideoStream\n",
    "from imutils.video import FPS\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "# construct the argument parse and parse the arguments\n",
    "# ap = argparse.ArgumentParser()\n",
    "# ap.add_argument(\"-p\", \"--prototxt\", required=True,\n",
    "# \thelp=\"path to Caffe 'deploy' prototxt file\")\n",
    "# ap.add_argument(\"-m\", \"--model\", required=True,\n",
    "# \thelp=\"path to Caffe pre-trained model\")\n",
    "# ap.add_argument(\"-c\", \"--confidence\", type=float, default=0.2,\n",
    "# \thelp=\"minimum probability to filter weak detections\")\n",
    "# args = vars(ap.parse_args())\n",
    "\n",
    "# initialize the list of class labels MobileNet SSD was trained to\n",
    "# detect, then generate a set of bounding box colors for each class\n",
    "CLASSES = [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\",\n",
    "\t\"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\",\n",
    "\t\"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\",\n",
    "\t\"sofa\", \"train\", \"tvmonitor\"]\n",
    "COLORS = np.random.uniform(0, 255, size=(len(CLASSES), 3))\n",
    "\n",
    "# load our serialized model from disk\n",
    "print(\"[INFO] loading model...\")\n",
    "# net = cv2.dnn.readNetFromCaffe(args[\"prototxt\"], args[\"model\"])\n",
    "net = cv2.dnn.readNetFromCaffe('MobileNetSSD_deploy.prototxt.txt', 'MobileNetSSD_deploy.caffemodel')\n",
    "\n",
    "# initialize the video stream, allow the cammera sensor to warmup,\n",
    "# and initialize the FPS counter\n",
    "print(\"[INFO] starting video stream...\")\n",
    "vs = VideoStream(src=0).start()\n",
    "time.sleep(2.0)\n",
    "fps = FPS().start()\n",
    "\n",
    "# loop over the frames from the video stream\n",
    "while True:\n",
    "\t# grab the frame from the threaded video stream and resize it\n",
    "\t# to have a maximum width of 400 pixels\n",
    "\tframe = vs.read()\n",
    "\tframe = imutils.resize(frame, width=400)\n",
    "\n",
    "\t# grab the frame dimensions and convert it to a blob\n",
    "\t(h, w) = frame.shape[:2]\n",
    "\tblob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)),\n",
    "\t\t0.007843, (300, 300), 127.5)\n",
    "\n",
    "\t# pass the blob through the network and obtain the detections and\n",
    "\t# predictions\n",
    "\tnet.setInput(blob)\n",
    "\tdetections = net.forward()\n",
    "\n",
    "\t# loop over the detections\n",
    "\tfor i in np.arange(0, detections.shape[2]):\n",
    "\t\t# extract the confidence (i.e., probability) associated with\n",
    "\t\t# the prediction\n",
    "\t\tconfidence = detections[0, 0, i, 2]\n",
    "\n",
    "\t\t# filter out weak detections by ensuring the `confidence` is\n",
    "\t\t# greater than the minimum confidence\n",
    "\t\tif confidence > 0.2:\n",
    "\t\t\t# extract the index of the class label from the\n",
    "\t\t\t# `detections`, then compute the (x, y)-coordinates of\n",
    "\t\t\t# the bounding box for the object\n",
    "\t\t\tidx = int(detections[0, 0, i, 1])\n",
    "\t\t\tbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "\t\t\t(startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "\t\t\t# draw the prediction on the frame\n",
    "\t\t\tlabel = \"{}: {:.2f}%\".format(CLASSES[idx],\n",
    "\t\t\t\tconfidence * 100)\n",
    "# \t\t\tcv2.rectangle(frame, (startX, startY), (endX, endY),\n",
    "# \t\t\t\tCOLORS[idx], 2)\n",
    "\t\t\ty = startY - 15 if startY - 15 > 15 else startY + 15\n",
    "\t\t\tcv2.putText(frame, label, (startX, y),\n",
    "\t\t\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.5, COLORS[idx], 2)\n",
    "\n",
    "\t# show the output frame\n",
    "\tcv2.imshow(\"Frame\", frame)\n",
    "\tkey = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "\t# if the `q` key was pressed, break from the loop\n",
    "\tif key == ord(\"q\"):\n",
    "\t\tbreak\n",
    "\n",
    "\t# update the FPS counter\n",
    "\tfps.update()\n",
    "\n",
    "# stop the timer and display FPS information\n",
    "fps.stop()\n",
    "print(\"[INFO] elapsed time: {:.2f}\".format(fps.elapsed()))\n",
    "print(\"[INFO] approx. FPS: {:.2f}\".format(fps.fps()))\n",
    "\n",
    "# do a bit of cleanup\n",
    "cv2.destroyAllWindows()\n",
    "vs.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7b98df223fab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[1;31m# the bounding box for the object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m             \u001b[0mbox\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetections\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[0mstartX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstartY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mendX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mendY\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbox\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"int\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'w' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import ImageGrab\n",
    "from imutils.video import VideoStream\n",
    "from imutils.video import FPS\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "cont = 0\n",
    "maxArea = 300\n",
    "prevC = 0\n",
    "danger = False\n",
    "bgs = cv2.createBackgroundSubtractorMOG2(10,80,True)\n",
    "net = cv2.dnn.readNetFromCaffe('MobileNetSSD_deploy.prototxt.txt', 'MobileNetSSD_deploy.caffemodel')\n",
    "\n",
    "while(True):\n",
    "    \n",
    "    ret, frame = cap.read()    \n",
    "    blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)),\t0.007843, (300, 300), 127.5)\n",
    "   \n",
    "\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "\n",
    "    # loop over the detections\n",
    "    for i in np.arange(0, detections.shape[2]):\n",
    "        # extract the confidence (i.e., probability) associated with\n",
    "        # the prediction\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "\n",
    "        # filter out weak detections by ensuring the `confidence` is\n",
    "        # greater than the minimum confidence\n",
    "        if confidence > 0.2:\n",
    "            # extract the index of the class label from the\n",
    "            # `detections`, then compute the (x, y)-coordinates of\n",
    "            # the bounding box for the object\n",
    "            idx = int(detections[0, 0, i, 1])\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "            # draw the prediction on the frame\n",
    "            label = \"{}: {:.2f}%\".format(CLASSES[idx],\n",
    "                confidence * 100)\n",
    "            cv2.rectangle(frame, (startX, startY), (endX, endY),\n",
    "                COLORS[idx], 2)\n",
    "            y = startY - 15 if startY - 15 > 15 else startY + 15\n",
    "            cv2.putText(frame, label, (startX, y),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLORS[idx], 2)\n",
    "\n",
    "    # show the output frame\n",
    "    cv2.imshow('frame',frame) \n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
